import pandas as pd
import numpy as np
import os
import random
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import average_precision_score
from tqdm import tqdm
import torch
from datetime import datetime
import pyarrow.parquet as pq
import gc

# ëª¨ë¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
import xgboost as xgb

# ===================================================================
# 1. ì„¤ì • (CONFIGURATIONS)
# ===================================================================
class CFG:
    # --- ê¸°ë³¸ ì„¤ì • ---
    SEED = 7
    N_FOLDS = 10
    TARGET_0_RATIO = 1.5

    # --- ê²½ë¡œ ì„¤ì • ---
    DATA_PATH = './data/toss/data/'
    SAVE_PATH = './data/toss/save/'
    TRAIN_PATH = os.path.join(DATA_PATH, "train.parquet")
    TEST_PATH = os.path.join(DATA_PATH, "test.parquet")
    SUBMISSION_PATH = os.path.join(DATA_PATH, 'sample_submission.csv')

    # --- ë°ì´í„° ë¡œë”© ë° ìƒ˜í”Œë§ ---
    BATCH_SIZE = 200000

    # --- í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ---
    TARGET_COL = "clicked"
    MAIN_CAT_FEATS = ['inventory_id', 'age_group', 'ad_id', 'carrier', 'platform', 'gender', 'day_of_week']
    MAIN_NUM_FEATS = ['history_a_1', 'l_feat_16', 'l_feat_1', 'history_b_2', 'l_feat_2', 'history_a_2', 'history_b_1']
    INTERACTION_PAIRS = [['inventory_id', 'age_group'], ['ad_id', 'platform'], ['ad_id', 'age_group']]
    EPSILON = 1e-6

    # --- ëª¨ë¸ í›ˆë ¨ ---
    SMOOTHING_FACTOR = 20
    XGB_PARAMS = {
        'objective': 'binary:logistic', 'eval_metric': 'aucpr',
        'tree_method': 'hist', 'device': 'cuda', 'random_state': SEED,
        'learning_rate': 0.01, 'max_depth': 4, 'subsample': 0.77,
        'colsample_bytree': 0.7, 'gamma': 0.03, 'lambda': 0.8, 'alpha': 0.1,
        'enable_categorical': True
    }
    NUM_BOOST_ROUND = 10000
    EARLY_STOPPING_ROUNDS = 100

# ===================================================================
# 2. ìœ í‹¸ë¦¬í‹° (Utilities)
# ===================================================================
def log_message(message):
    current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"[{current_time}] {message}")

def reduce_mem_usage(df, verbose=True):
    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']
    start_mem = df.memory_usage().sum() / 1024**2
    for col in df.columns:
        col_type = df[col].dtypes
        if col_type in numerics:
            c_min = df[col].min()
            c_max = df[col].max()
            if str(col_type)[:3] == 'int':
                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max: df[col] = df[col].astype(np.int8)
                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max: df[col] = df[col].astype(np.int16)
                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max: df[col] = df[col].astype(np.int32)
                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max: df[col] = df[col].astype(np.int64)
            else:
                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max: df[col] = df[col].astype(np.float32)
                else: df[col] = df[col].astype(np.float64)
    end_mem = df.memory_usage().sum() / 1024**2
    if verbose: log_message(f'Memory usage reduced to {end_mem:.2f} MB ({100 * (start_mem - end_mem) / start_mem:.1f}% reduction)')
    return df

def seed_everything(seed):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.backends.cudnn.benchmark = False
        torch.backends.cudnn.deterministic = True

# ===================================================================
# 3. ë©”ì¸ ë¡œì§ (Main Logic)
# ===================================================================
seed_everything(CFG.SEED)
os.makedirs(CFG.SAVE_PATH, exist_ok=True)

log_message("í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë”©...")
test = pd.read_parquet(CFG.TEST_PATH, engine="pyarrow").drop(columns=['ID'])

log_message("í›ˆë ¨ ë°ì´í„° ìƒ˜í”Œë§ ì‹œì‘...")
pf = pq.ParquetFile(CFG.TRAIN_PATH)
n_rows = pf.metadata.num_rows
n_clicked_1 = 0
for chunk in tqdm(pf.iter_batches(batch_size=CFG.BATCH_SIZE, columns=['clicked']), total=-(-n_rows//CFG.BATCH_SIZE), desc="Target ì¹´ìš´íŒ…"):
    n_clicked_1 += chunk.to_pandas()['clicked'].sum()

n_clicked_0 = n_rows - n_clicked_1
target_0_samples = n_clicked_1 * CFG.TARGET_0_RATIO
sample_ratio_0 = target_0_samples / n_clicked_0 if n_clicked_0 > 0 else 0

sampled_chunks = []
for chunk in tqdm(pf.iter_batches(batch_size=CFG.BATCH_SIZE), total=-(-n_rows//CFG.BATCH_SIZE), desc="ë°ì´í„° ë¡œë”© ë° ìƒ˜í”Œë§"):
    df_chunk = chunk.to_pandas()
    df_1 = df_chunk[df_chunk['clicked'] == 1]
    df_0 = df_chunk[df_chunk['clicked'] == 0]
    df_0_sampled = df_0.sample(frac=sample_ratio_0, random_state=CFG.SEED)
    sampled_chunks.append(pd.concat([df_1, df_0_sampled]))

train_df = pd.concat(sampled_chunks, ignore_index=True).sample(frac=1, random_state=CFG.SEED).reset_index(drop=True)
del pf, sampled_chunks, df_chunk, df_1, df_0, df_0_sampled
gc.collect()

log_message("í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì‹œì‘...")
FEATURE_EXCLUDE = {CFG.TARGET_COL, "seq", "ID"}
train_len = len(train_df)

y_target = train_df[CFG.TARGET_COL].copy()
combined_df = pd.concat([train_df.drop(columns=[CFG.TARGET_COL]), test], ignore_index=True)

combined_df['hour'] = pd.to_numeric(combined_df['hour'], errors='coerce').astype(int)
combined_df['time_of_day'] = pd.cut(combined_df['hour'], bins=[-1, 6, 12, 18, 23], labels=[0, 1, 2, 3], ordered=False).astype(int)

existing_cat_feats = [col for col in CFG.MAIN_CAT_FEATS if col in combined_df.columns]
existing_num_feats = [col for col in CFG.MAIN_NUM_FEATS if col in combined_df.columns]

# ### [ê°œì„  3] ë¹ˆë„ ì¸ì½”ë”© (Frequency Encoding) ###
log_message("ë¹ˆë„ ì¸ì½”ë”© í”¼ì²˜ ìƒì„±...")
for col in tqdm(existing_cat_feats, desc="ë¹ˆë„ ì¸ì½”ë”©"):
    freq_map = combined_df[col].value_counts(normalize=True)
    combined_df[f'{col}_freq'] = combined_df[col].map(freq_map)
    
log_message("í™•ì¥ëœ ê·¸ë£¹ í†µê³„ í”¼ì²˜ ìƒì„±...")
aggs = ['mean', 'std', 'min', 'max', 'median', 'nunique']
for cat_feat in tqdm(existing_cat_feats, desc="ë²”ì£¼í˜• í”¼ì²˜ ë£¨í”„"):
    for num_feat in existing_num_feats:
        for agg in aggs:
            group_agg = combined_df.groupby(cat_feat)[num_feat].transform(agg)
            combined_df[f'{cat_feat}_{num_feat}_{agg}'] = group_agg
        combined_df[f'{cat_feat}_{num_feat}_mean_diff'] = combined_df[num_feat] - combined_df[f'{cat_feat}_{num_feat}_mean']

log_message("ê³ ì°¨ì› ìƒí˜¸ì‘ìš© í”¼ì²˜ ìƒì„±...")
for pair in tqdm(CFG.INTERACTION_PAIRS, desc="ê³ ì°¨ì› ìƒí˜¸ì‘ìš© ë£¨í”„"):
    if all(c in combined_df.columns for c in pair):
        group_col_name = '_'.join(pair)
        combined_df[group_col_name] = combined_df[pair[0]].astype(str) + '_' + combined_df[pair[1]].astype(str)
        for num_feat in existing_num_feats:
            group = combined_df.groupby(group_col_name)[num_feat]
            combined_df[f'{group_col_name}_{num_feat}_mean'] = group.transform('mean')
            combined_df[f'{group_col_name}_{num_feat}_std'] = group.transform('std')
        combined_df.drop(columns=[group_col_name], inplace=True)

log_message("ê¸°ë³¸ ìƒí˜¸ì‘ìš© ë° ë¹„ìœ¨ í”¼ì²˜ ìƒì„±...")
if 'history_a_1' in combined_df.columns and 'history_b_2' in combined_df.columns:
    combined_df['history_a1_div_b2'] = combined_df['history_a_1'] / (combined_df['history_b_2'] + CFG.EPSILON)
if 'l_feat_1' in combined_df.columns and 'l_feat_2' in combined_df.columns:
    combined_df['l_feat_1_div_2'] = combined_df['l_feat_1'] / (combined_df['l_feat_2'] + CFG.EPSILON)

train_df = combined_df[:train_len].copy()
test = combined_df[train_len:].copy()
train_df[CFG.TARGET_COL] = y_target
del combined_df, y_target
gc.collect()

log_message("ë°ì´í„° íƒ€ì… ë³€í™˜ (object -> category)...")
categorical_cols = [c for c in train_df.columns if train_df[c].dtype == 'object']
for col in tqdm(categorical_cols):
    train_df[col] = train_df[col].astype('category')
    test[col] = test[col].astype('category')

log_message("ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ê°•í™”...")
train_df.replace([np.inf, -np.inf], np.nan, inplace=True)
test.replace([np.inf, -np.inf], np.nan, inplace=True)
numeric_cols = train_df.select_dtypes(include=np.number).columns.tolist()
numeric_cols.remove(CFG.TARGET_COL) # íƒ€ê²Ÿ ì»¬ëŸ¼ì€ ì œì™¸
for col in tqdm(numeric_cols, desc="ê²°ì¸¡ì¹˜ ì¤‘ì•™ê°’ ëŒ€ì²´"):
    median_val = train_df[col].median()
    train_df[col].fillna(median_val, inplace=True)
    test[col].fillna(median_val, inplace=True)

# ### [ê°œì„  5] OOF ìŠ¤ë¬´ë”© íƒ€ê²Ÿ ì¸ì½”ë”© (OOF Smoothing Target Encoding) ###
log_message("OOF ìŠ¤ë¬´ë”© íƒ€ê²Ÿ ì¸ì½”ë”© ì‹œì‘...")
skf_te = StratifiedKFold(n_splits=CFG.N_FOLDS, shuffle=True, random_state=CFG.SEED)
global_mean = train_df[CFG.TARGET_COL].mean()

for col in tqdm(existing_cat_feats, desc="OOF íƒ€ê²Ÿ ì¸ì½”ë”©"):
    te_col_name = f'{col}_te'
    train_df[te_col_name] = np.nan

    # í›ˆë ¨ ë°ì´í„°ì— ëŒ€í•œ OOF ì¸ì½”ë”© ê°’ ê³„ì‚°
    for train_idx, val_idx in skf_te.split(train_df, train_df[CFG.TARGET_COL]):
        X_train, X_val = train_df.iloc[train_idx], train_df.iloc[val_idx]
        
        grouped = X_train.groupby(X_train[col].astype(str))[CFG.TARGET_COL]
        means = grouped.mean()
        counts = grouped.count()
        
        smoothed_means = (counts * means + CFG.SMOOTHING_FACTOR * global_mean) / (counts + CFG.SMOOTHING_FACTOR)
        
        train_df.loc[val_idx, te_col_name] = X_val[col].astype(str).map(smoothed_means)

    train_df[te_col_name].fillna(global_mean, inplace=True)

    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¸ì½”ë”©ì„ ìœ„í•´ ì „ì²´ í›ˆë ¨ ë°ì´í„°ë¥¼ ì‚¬ìš©
    grouped_full = train_df.groupby(train_df[col].astype(str))[CFG.TARGET_COL]
    means_full = grouped_full.mean()
    counts_full = grouped_full.count()
    smoothed_means_full = (counts_full * means_full + CFG.SMOOTHING_FACTOR * global_mean) / (counts_full + CFG.SMOOTHING_FACTOR)
    
    test[te_col_name] = test[col].astype(str).map(smoothed_means_full).fillna(global_mean)

log_message("OOF ìŠ¤ë¬´ë”© íƒ€ê²Ÿ ì¸ì½”ë”© ì™„ë£Œ.")
    
# ### [ê°œì„  4] ê²€ì¦ ì „ëµì— ëŒ€í•œ ê³ ì°° ###
# í˜„ì¬ ë°ì´í„°ì…‹ì—ëŠ” user_idì™€ ê°™ì´ ì‚¬ìš©ìë¥¼ íŠ¹ì •í•  ìˆ˜ ìˆëŠ” ëª…í™•í•œ ê·¸ë£¹í•‘ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.
# ì´ëŸ¬í•œ ê²½ìš°, StratifiedKFoldëŠ” ê° Foldì˜ íƒ€ê²Ÿ ë³€ìˆ˜(clicked) ë¹„ìœ¨ì„ ì „ì²´ ë°ì´í„°ì…‹ê³¼ ìœ ì‚¬í•˜ê²Œ ìœ ì§€ì‹œì¼œì£¼ë¯€ë¡œ,
# í´ë˜ìŠ¤ ë¶ˆê· í˜• ìƒí™©ì—ì„œ ì•ˆì •ì ì´ê³  ì‹ ë¢°ë„ ë†’ì€ ê²€ì¦ ì„±ëŠ¥ì„ ì¸¡ì •í•  ìˆ˜ ìˆëŠ” ë§¤ìš° íš¨ê³¼ì ì¸ ë°©ë²•ì…ë‹ˆë‹¤.
# ë”°ë¼ì„œ í˜„ì¬ì˜ ê²€ì¦ ì „ëµì„ ìœ ì§€í•©ë‹ˆë‹¤.

log_message("í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì™„ë£Œ.")

# ì‚¬ìš©í•  ëª¨ë“  í”¼ì²˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
feature_cols = [c for c in train_df.columns if c not in FEATURE_EXCLUDE]


# ===================================================================
# 4. ëª¨ë¸ í›ˆë ¨ ë° ì œì¶œ (Model Training & Submission)
# ===================================================================
if __name__ == '__main__':
    y = train_df[CFG.TARGET_COL].copy()
    X = train_df[feature_cols].copy()
    del train_df
    gc.collect()

    log_message("XGBoost ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")

    skf = StratifiedKFold(n_splits=CFG.N_FOLDS, shuffle=True, random_state=CFG.SEED)
    models = []
    oof_scores = []
    
    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):
        log_message(f"======== Fold {fold}/{CFG.N_FOLDS} í›ˆë ¨ ì‹œì‘ ========")
        X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]
        X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]
        
        # OOF íƒ€ê²Ÿ ì¸ì½”ë”©ì´ ì‚¬ì „ì— ì™„ë£Œë˜ì—ˆìœ¼ë¯€ë¡œ, ì—¬ê¸°ì„œëŠ” ì¶”ê°€ì ì¸ ì¸ì½”ë”©ì´ í•„ìš” ì—†ìŠµë‹ˆë‹¤.
        
        dtrain = xgb.DMatrix(X_train, label=y_train, enable_categorical=True)
        dval = xgb.DMatrix(X_val, label=y_val, enable_categorical=True)
        
        model_params = CFG.XGB_PARAMS.copy()
        model_params['scale_pos_weight'] = (len(y_train) - y_train.sum()) / (y_train.sum() + CFG.EPSILON)
        
        model = xgb.train(
            params=model_params, dtrain=dtrain, num_boost_round=CFG.NUM_BOOST_ROUND,
            evals=[(dval, 'val')], early_stopping_rounds=CFG.EARLY_STOPPING_ROUNDS, verbose_eval=500
        )
        
        oof_preds = model.predict(dval, iteration_range=(0, model.best_iteration))
        fold_score = average_precision_score(y_val, oof_preds)
        oof_scores.append(fold_score)
        log_message(f"Fold {fold} AUC PR Score: {fold_score:.5f}")
        
        models.append(model)
        del dtrain, dval, X_train, y_train, X_val, y_val
        gc.collect()

    mean_oof_score = np.mean(oof_scores)
    log_message(f"ğŸ“ˆ Mean OOF AUC PR Score (ë¡œì»¬ ì ìˆ˜): {mean_oof_score:.5f}")

    log_message("í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ë° ì œì¶œ íŒŒì¼ ìƒì„± ì‹œì‘...")
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ íƒ€ê²Ÿ ì¸ì½”ë”©ì´ ì‚¬ì „ì— ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.
    model_feature_names = models[0].feature_names
    test_processed = test[model_feature_names]

    dtest_xgb = xgb.DMatrix(test_processed, enable_categorical=True)
    xgb_preds_list = [model.predict(dtest_xgb, iteration_range=(0, model.best_iteration)) for model in models]
    final_predictions = np.mean(xgb_preds_list, axis=0)
    
    submission = pd.read_csv(CFG.SUBMISSION_PATH)
    submission[CFG.TARGET_COL] = final_predictions
    
    current_time_str = datetime.now().strftime("%m%d_%H%M")
    submission_filename = f"submission_{current_time_str}_xgb_v2_oof_{mean_oof_score:.5f}.csv"
    submission_filepath = os.path.join(CFG.SAVE_PATH, submission_filename)
    
    submission.to_csv(submission_filepath, index=False)
    log_message(f"âœ… ì œì¶œ íŒŒì¼ ìƒì„±ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤: {submission_filepath}")


#ğŸ“ˆ Mean OOF AUC PR Scoe (ë¡œì»¬ ì ìˆ˜): 0.67292
#í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ë° ì œì¶œ íŒŒì¼ ìƒì„± ì‹œì‘...
#âœ… ì œì¶œ íŒŒì¼ ìƒì„±ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤: ./Dacon/toss/_save/submission_1011_1643_xgb_v2_oof_0.67292.csv